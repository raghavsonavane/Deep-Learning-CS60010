{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import data_loader as dl\n",
    "import module\n",
    "import os\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data import vision\n",
    "from multiprocessing import cpu_count\n",
    "mx.random.seed(42)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data, label):\n",
    "    data = data.astype('float32')/255\n",
    "    return data, label\n",
    "\n",
    "def shuffle_dataset(X,Y):\n",
    "    \n",
    "    '''\n",
    "        Write code to shuffle the dataset here. \n",
    "        \n",
    "        Args: \n",
    "            X: Input feature ndarray\n",
    "            Y: Input values ndarray\n",
    "            \n",
    "        Return:\n",
    "            X and Y shuffled in place\n",
    "    \n",
    "    '''\n",
    "    r = np.arange(len(X))\n",
    "    np.random.shuffle(r)\n",
    "    X = X[r]\n",
    "    Y = Y[r]\n",
    "    return (X,Y)\n",
    "    pass\n",
    "\n",
    "class DataIterLoader():\n",
    "    def __init__(self, data_iter):\n",
    "        self.data_iter = data_iter\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.data_iter.reset()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = self.data_iter.__next__()\n",
    "        assert len(batch.data) == len(batch.label) == 1\n",
    "        data = batch.data[0]\n",
    "        label = batch.label[0]\n",
    "        return data, label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images,train_labels)=dl.DataLoader().load_data('train')\n",
    "(test_images,test_labels)=dl.DataLoader().load_data('test')\n",
    "batch_size=32\n",
    "\n",
    "ntrain=int(0.7*len(train_images))\n",
    "#Normalization of input pixel data\n",
    "train_iter = mx.io.NDArrayIter(train_images[:ntrain, :]/255, train_labels[:ntrain], batch_size, shuffle=True)\n",
    "val_iter = mx.io.NDArrayIter(train_images[ntrain:, :]/255, train_labels[ntrain:], batch_size)\n",
    "test_iter = mx.io.NDArrayIter(test_images/255, test_labels, batch_size)\n",
    "\n",
    "train_loader = DataIterLoader(train_iter)\n",
    "val_loader = DataIterLoader(val_iter)\n",
    "test_loader = DataIterLoader(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn.HybridSequential()\n",
    "\n",
    "with net2.name_scope():\n",
    "    net2.add(\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(1024, activation='relu'),\n",
    "        nn.Dense(512, activation='relu'),\n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dense(10, activation=None)  # loss function includes softmax already, see below\n",
    "    )\n",
    "net2.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_training(net,trainer,ctx):\n",
    "    epochs = 10\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    epoch_history = []\n",
    "    for epoch in range(epochs):\n",
    "        # training loop (with autograd and trainer steps, etc.)\n",
    "        cumulative_train_loss = mx.nd.zeros(1, ctx=ctx)\n",
    "        training_samples = 0\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            data = data.as_in_context(ctx).reshape((-1, 784)) # 28*28=784\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = loss_function(output, label)\n",
    "            loss.backward()\n",
    "            metric.update(label, output)\n",
    "            trainer.step(data.shape[0])\n",
    "            cumulative_train_loss += loss.sum()\n",
    "            training_samples += data.shape[0]\n",
    "\n",
    "\n",
    "        train_loss = cumulative_train_loss.asscalar()/training_samples\n",
    "        train_name, train_acc = metric.get()\n",
    "        #print('After epoch {}: {} = {}'.format(epoch + 1, name, acc))\n",
    "        metric.reset()\n",
    "\n",
    "        # validation loop\n",
    "        cumulative_val_loss = mx.nd.zeros(1, ctx)\n",
    "        val_samples = 0\n",
    "        for batch_idx, (data, label) in enumerate(val_loader):\n",
    "            data = data.as_in_context(ctx).reshape((-1, 784)) # 28*28=784\n",
    "            label = label.as_in_context(ctx)\n",
    "            output = net(data)\n",
    "            loss = loss_function(output, label)\n",
    "            cumulative_val_loss += loss.sum()\n",
    "            val_samples += data.shape[0]\n",
    "            metric.update(label, output)\n",
    "        val_loss = cumulative_val_loss.asscalar()/val_samples\n",
    "        val_name, val_acc = metric.get()\n",
    "        metric.reset()\n",
    "\n",
    "        print(\"Epoch {}, training loss: {:.2f}, validation loss: {:.2f}\".format(epoch, train_loss, val_loss))\n",
    "        print(\"training accuracy: {}, validation accuracy: {}\".format(train_acc,val_acc))\n",
    "        train_loss_history.append(train_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "        epoch_history.append(epoch+1)\n",
    "#     plt.plot(epoch_history, train_loss_history, epoch_history, val_loss_history )\n",
    "#     plt.title('Loss')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot(epoch_history, train_acc_history, epoch_history, val_acc_history )\n",
    "#     plt.title('Accuracy')\n",
    "#     plt.show()\n",
    "    return epoch_history,train_loss_history,val_loss_history,train_acc_history,val_acc_history\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_testing(net,ctx):\n",
    "    cumulative_test_loss = mx.nd.zeros(1, ctx)\n",
    "    test_samples = 0\n",
    "    for batch_idx, (data, label) in enumerate(test_loader):\n",
    "            data = data.as_in_context(ctx).reshape((-1, 784)) # 28*28=784\n",
    "            label = label.as_in_context(ctx)\n",
    "            output = net(data)\n",
    "            loss = loss_function(output, label)\n",
    "            cumulative_test_loss += loss.sum()\n",
    "            test_samples += data.shape[0]\n",
    "            metric.update(label, output)\n",
    "    test_loss = cumulative_test_loss.asscalar()/test_samples\n",
    "    test_name, test_acc = metric.get()\n",
    "    print(\"Final testing loss: {:.2f}\".format(test_loss))\n",
    "    print(\"testing accuracy: {}\".format(test_acc))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting internal features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'hybridsequential3_dense0_weight',\n",
       " 'hybridsequential3_dense0_bias',\n",
       " 'hybridsequential3_dense0_fwd_output',\n",
       " 'hybridsequential3_dense0_relu_fwd_output',\n",
       " 'hybridsequential3_dense1_weight',\n",
       " 'hybridsequential3_dense1_bias',\n",
       " 'hybridsequential3_dense1_fwd_output',\n",
       " 'hybridsequential3_dense1_relu_fwd_output',\n",
       " 'hybridsequential3_dense2_weight',\n",
       " 'hybridsequential3_dense2_bias',\n",
       " 'hybridsequential3_dense2_fwd_output',\n",
       " 'hybridsequential3_dense2_relu_fwd_output',\n",
       " 'hybridsequential3_dense3_weight',\n",
       " 'hybridsequential3_dense3_bias',\n",
       " 'hybridsequential3_dense3_fwd_output']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=mx.sym.load('vanilla_net2-symbol.json')\n",
    "internals=model.get_internals()\n",
    "ls=internals.list_outputs()\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "int1=internals[ls[4]]\n",
    "sym1 = gluon.nn.SymbolBlock(outputs=int1, inputs=mx.sym.var('data'))\n",
    "sym1.collect_params().load('vanilla_net2-0000.params', ctx=mx.cpu(0), ignore_extra=True)\n",
    "hidden_training1=sym1(mx.nd.array(train_images)).asnumpy()\n",
    "hidden_testing1=sym1(mx.nd.array(test_images)).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2=internals[ls[8]]\n",
    "sym2 = gluon.nn.SymbolBlock(outputs=int2, inputs=mx.sym.var('data'))\n",
    "sym2.collect_params().load('vanilla_net2-0000.params', ctx=mx.cpu(0), ignore_extra=True)\n",
    "hidden_training2=sym2(mx.nd.array(train_images)).asnumpy()\n",
    "hidden_testing2=sym2(mx.nd.array(test_images)).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "int3=internals[ls[12]]\n",
    "sym3 = gluon.nn.SymbolBlock(outputs=int3, inputs=mx.sym.var('data'))\n",
    "sym3.collect_params().load('vanilla_net2-0000.params', ctx=mx.cpu(0), ignore_extra=True)\n",
    "hidden_training3=sym3(mx.nd.array(train_images)).asnumpy()\n",
    "hidden_testing3=sym3(mx.nd.array(test_images)).asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with original data => training: 0.8632 and testing: 0.8412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf0 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(train_images, train_labels)\n",
    "a0=clf0.score(train_images,train_labels)\n",
    "b0=clf0.score(test_images,test_labels)\n",
    "print('Accuracy with original data => training: {} and testing: {}'.format(a0,b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 1st hidden layer data => training: 0.92465 and testing: 0.8794\n"
     ]
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(hidden_training1, train_labels)\n",
    "a1=clf1.score(hidden_training1,train_labels)\n",
    "b1=clf1.score(hidden_testing1,test_labels)\n",
    "print('Accuracy with 1st hidden layer data => training: {} and testing: {}'.format(a1,b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 2nd hidden layer data => training: 0.9303 and testing: 0.8859\n"
     ]
    }
   ],
   "source": [
    "clf2 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(hidden_training2, train_labels)\n",
    "a2=clf2.score(hidden_training2,train_labels)\n",
    "b2=clf2.score(hidden_testing2,test_labels)\n",
    "print('Accuracy with 2nd hidden layer data => training: {} and testing: {}'.format(a2,b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 3rd hidden layer data => training: 0.92655 and testing: 0.8886\n"
     ]
    }
   ],
   "source": [
    "clf3 = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(hidden_training3, train_labels)\n",
    "a3=clf3.score(hidden_training3,train_labels)\n",
    "b3=clf3.score(hidden_testing3,test_labels)\n",
    "print('Accuracy with 3rd hidden layer data => training: {} and testing: {}'.format(a3,b3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
